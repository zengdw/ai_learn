{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zengdw/ai_learn/blob/main/langchain/langChain_Groq.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "uxGtarhQnyi4",
        "outputId": "fa29854a-a773-4456-e951-81d32dc28a40"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/415.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.4/415.4 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m415.4/415.4 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m27.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.9/121.9 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -qU \"langchain[groq]\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import getpass\n",
        "import os\n",
        "\n",
        "groq_key = getpass.getpass(\"Groq API Key:\")\n",
        "os.environ[\"GROQ_API_KEY\"] = groq_key\n",
        "os.environ['LANGSMITH_ENDPOINT'] = 'https://api.smith.langchain.com'\n",
        "os.environ[\"LANGSMITH_TRACING\"] = 'True'\n",
        "os.environ['LANGSMITH_API_KEY'] = getpass.getpass(\"LangSmith API Key:\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rMGl6RAIxfOQ",
        "outputId": "b1b29e7a-1e1b-4f83-84ad-25366cc66c81"
      },
      "execution_count": 29,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Groq API Key:··········\n",
            "LangSmith API Key:··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Groq Api使用示例"
      ],
      "metadata": {
        "id": "lmZ7ytct3SWg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from groq import Groq\n",
        "\n",
        "client = Groq(api_key=groq_key)\n",
        "\n",
        "chat_completion = client.chat.completions.create(\n",
        "  messages=[\n",
        "    {\n",
        "      \"role\": \"user\",\n",
        "      \"content\": \"Explain the importance of fast language models\",\n",
        "    }\n",
        "  ],\n",
        "  model=\"llama-3.3-70b-versatile\",\n",
        ")\n",
        "\n",
        "print(chat_completion.choices[0].message.content)"
      ],
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ls86TcHXvaRZ",
        "outputId": "b25bcbf6-4874-4908-bfb6-c59904050985"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fast language models are crucial in the field of natural language processing (NLP) as they have numerous applications and benefits. Here are some of the key reasons that highlight the importance of fast language models:\n",
            "\n",
            "1. **Real-time Processing**: Fast language models enable real-time processing of text data, which is essential for applications such as chatbots, voice assistants, and language translation software. These models can process and respond to user input quickly, providing a seamless user experience.\n",
            "2. **Improved User Experience**: Fast language models can analyze and generate human-like text at a rapid pace, allowing for more efficient and effective communication. This is particularly important in customer service, where quick and accurate responses can significantly improve customer satisfaction.\n",
            "3. **Efficient Data Analysis**: Fast language models can quickly process large volumes of text data, making them ideal for tasks such as sentiment analysis, text classification, and information retrieval. This enables businesses to gain insights and make informed decisions based on their data.\n",
            "4. **Enhanced Decision-Making**: Fast language models can analyze vast amounts of text data, providing valuable insights and recommendations that can inform business decisions. For example, in the medical field, fast language models can quickly analyze medical texts to identify potential diagnoses or recommend treatment options.\n",
            "5. **Competitive Advantage**: Organizations that adopt fast language models can gain a competitive advantage by processing and analyzing text data more efficiently and effectively than their competitors. This can lead to improved customer engagement, better decision-making, and increased revenue.\n",
            "6. **Reduced Costs**: Fast language models can automate many tasks that would otherwise require manual processing, reducing the need for human intervention and minimizing the associated costs.\n",
            "7. **Increased Accessibility**: Fast language models can facilitate communication across languages and cultures, enabling people to access information and services that may have been previously inaccessible due to language barriers.\n",
            "8. **Support for Multimodal Interactions**: Fast language models can process and respond to multimodal inputs, such as voice, text, and gestures, allowing for more natural and intuitive human-computer interactions.\n",
            "\n",
            "Some of the key applications of fast language models include:\n",
            "\n",
            "1. **Virtual Assistants**: Fast language models power virtual assistants like Siri, Alexa, and Google Assistant, enabling them to quickly process and respond to user requests.\n",
            "2. **Language Translation**: Fast language models can quickly translate text from one language to another, facilitating communication across languages and cultures.\n",
            "3. **Chatbots**: Fast language models enable chatbots to analyze and respond to user input in real-time, providing efficient and effective customer support.\n",
            "4. **Sentiment Analysis**: Fast language models can quickly analyze text data to determine sentiment and emotions, providing valuable insights for businesses and organizations.\n",
            "5. **Text Summarization**: Fast language models can summarize long pieces of text into concise and meaningful summaries, saving users time and effort.\n",
            "\n",
            "In summary, fast language models have revolutionized the field of NLP by enabling real-time processing, efficient data analysis, and improved user experiences. Their applications are diverse and continue to grow, making them a crucial component of many modern technologies and industries.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "\n",
        "client = openai.OpenAI(\n",
        "  base_url=\"https://api.groq.com/openai/v1\",\n",
        "  api_key=groq_key\n",
        ")\n",
        "\n",
        "chat_completion = client.chat.completions.create(\n",
        "  messages=[\n",
        "    {\n",
        "      \"role\": \"user\",\n",
        "      \"content\": \"Explain the importance of fast language models\",\n",
        "    }\n",
        "  ],\n",
        "  model=\"llama-3.3-70b-versatile\",\n",
        ")\n",
        "\n",
        "print(chat_completion.choices[0].message.content)"
      ],
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u4_Q02SAw67j",
        "outputId": "f9809126-a567-4955-d126-e880f95839ff"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fast language models are crucial in natural language processing (NLP) as they enable efficient and effective text analysis, generation, and understanding. The importance of fast language models can be seen in several aspects:\n",
            "\n",
            "1. **Real-time Applications**: Fast language models allow for real-time processing of text data, which is essential for applications such as chatbots, virtual assistants, and language translation software. These models can quickly respond to user input, providing instant feedback and enhancing the user experience.\n",
            "2. **Large-Scale Text Analysis**: With the increasing amount of text data available, fast language models can efficiently process and analyze large volumes of text, making them ideal for tasks such as sentiment analysis, entity recognition, and text classification.\n",
            "3. **Improved User Experience**: Fast language models can reduce latency and improve response times, leading to a better user experience. For instance, language models used in search engines can quickly provide relevant results, while those used in content generation can rapidly produce high-quality text.\n",
            "4. **Enhanced Accuracy**: With the ability to process large amounts of data quickly, fast language models can learn from extensive datasets, leading to improved accuracy in tasks such as language translation, text summarization, and question answering.\n",
            "5. **Increased Productivity**: Fast language models can automate many tasks, such as data preprocessing, feature extraction, and model training, freeing up time for developers and researchers to focus on higher-level tasks and improving overall productivity.\n",
            "6. **Edge AI and IoT Applications**: Fast language models are essential for edge AI and Internet of Things (IoT) applications, where devices have limited computational resources and require efficient processing of text data to perform tasks such as speech recognition, text classification, and sentiment analysis.\n",
            "7. **Low-Latency Requirements**: Fast language models are necessary for applications with low-latency requirements, such as real-time speech recognition, live captioning, and instant messaging, where delays can lead to poor user experience.\n",
            "8. **Scalability**: Fast language models can handle large volumes of text data and scale to meet the demands of growing applications, making them ideal for use cases such as customer service chatbots, language translation platforms, and content generation services.\n",
            "9. **Reduced Computational Resources**: Fast language models can reduce the computational resources required for text analysis and generation, making them more energy-efficient and cost-effective, which is particularly important for devices with limited resources, such as smartphones and smart home devices.\n",
            "10. **Advancements in NLP Research**: Fast language models can accelerate NLP research by enabling researchers to quickly experiment with new ideas, test hypotheses, and explore large datasets, leading to new breakthroughs and advancements in the field.\n",
            "\n",
            "In summary, fast language models are essential for a wide range of applications, from real-time text analysis and generation to large-scale text processing and edge AI. Their importance lies in their ability to provide efficient, accurate, and scalable text processing, which is critical for many industries and use cases.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Langchain Groq 使用"
      ],
      "metadata": {
        "id": "K_OcB7qe3ZZL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chat_models import init_chat_model\n",
        "from langchain_core.messages import SystemMessage, HumanMessage\n",
        "\n",
        "model = init_chat_model('llama-3.3-70b-versatile', model_provider = 'groq')\n",
        "\n",
        "# langchain消息格式调用\n",
        "messages = [\n",
        "  SystemMessage(\"Translate the following from English into Italian\"),\n",
        "  HumanMessage(\"hi!\"),\n",
        "]\n",
        "model.invoke(messages)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "uTBgublFy4LQ",
        "outputId": "e87ee569-7009-443f-c6d8-8810eec9ae5a"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='Ciao!', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 4, 'prompt_tokens': 44, 'total_tokens': 48, 'completion_time': 0.014545455, 'prompt_time': 0.005701137, 'queue_time': 0.902793238, 'total_time': 0.020246592}, 'model_name': 'llama-3.3-70b-versatile', 'system_fingerprint': 'fp_5f849c5a0b', 'finish_reason': 'stop', 'logprobs': None}, id='run-3124f280-3f17-42f4-905c-f49260edf902-0', usage_metadata={'input_tokens': 44, 'output_tokens': 4, 'total_tokens': 48})"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# OpenAi消息格式调用\n",
        "model.invoke([{\"role\": \"user\", \"content\": \"Hello\"}])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "JLlr0ZuRzWFK",
        "outputId": "138088d1-af8a-499b-b781-e8e6e473e4d9"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='Hello. How can I assist you today?', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 10, 'prompt_tokens': 36, 'total_tokens': 46, 'completion_time': 0.036363636, 'prompt_time': 0.004433405, 'queue_time': 0.391910282, 'total_time': 0.040797041}, 'model_name': 'llama-3.3-70b-versatile', 'system_fingerprint': 'fp_5d5eebccf4', 'finish_reason': 'stop', 'logprobs': None}, id='run-8c5af37a-13a6-4ace-9a0c-cfd3c02ace70-0', usage_metadata={'input_tokens': 36, 'output_tokens': 10, 'total_tokens': 46})"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 字符串格式调用\n",
        "model.invoke(\"Hello\")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "y8QA8-Rc5nPR",
        "outputId": "546dd46e-90b0-4fc5-eb54-cd9dcd3c89ab",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='Hello. How can I help you today?', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 10, 'prompt_tokens': 36, 'total_tokens': 46, 'completion_time': 0.036822332, 'prompt_time': 0.004644242, 'queue_time': 0.19228193500000001, 'total_time': 0.041466574}, 'model_name': 'llama-3.3-70b-versatile', 'system_fingerprint': 'fp_5f849c5a0b', 'finish_reason': 'stop', 'logprobs': None}, id='run-a9beeeff-4f9c-4295-b03f-0147aceeae28-0', usage_metadata={'input_tokens': 36, 'output_tokens': 10, 'total_tokens': 46})"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 流式输出\n",
        "for token in model.stream(\"hello\"):\n",
        "  print(token.content, end=\"\")"
      ],
      "metadata": {
        "id": "gTxyOLEa5p5Z",
        "outputId": "3e748155-61d0-4c8c-e2ee-686b36320328",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello. How can I help you today?"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 提示模板"
      ],
      "metadata": {
        "id": "FWyhLB4G6k8R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from ast import mod\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "system_template = \"Translate the following from English into {language}\"\n",
        "prompt_template = ChatPromptTemplate.from_messages(\n",
        "  [(\"system\", system_template), (\"user\", \"{text}\")]\n",
        ")\n",
        "prompt = prompt_template.invoke({\"language\": \"Chinese\", \"text\": \"I love programming\"})\n",
        "print(f'prmopt: {prompt}')\n",
        "res = model.invoke(prompt)\n",
        "print(f'res: {res.content}')"
      ],
      "metadata": {
        "id": "iLpOMmKh6DZq",
        "outputId": "5e825f6c-c476-48e5-d7e5-f274bec9dc47",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prmopt: messages=[SystemMessage(content='Translate the following from English into Chinese', additional_kwargs={}, response_metadata={}), HumanMessage(content='I love programming', additional_kwargs={}, response_metadata={})]\n",
            "res: 我爱编程 (wǒ ài biān chéng)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mbZGNzIr7NWJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "python3.10",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}